# ============================================================
# Systematic Review Pipeline — RTX 5090 / High-End Profile
# ============================================================
# Optimized for: 32GB VRAM, 64GB RAM, i9 13th gen
# Usage: python src/main.py --local --profile 5090 --taxonomy config/biodiesel_prompts.json
# ============================================================

# ------ LLM Settings ------
llm:
  provider: ollama
  base_url: "http://localhost:11434"
  model: "qwen3:8b"               # Default model for general tasks
  temperature: 0.2
  seed: 42
  timeout: 900                        # Max seconds of silence between streamed chunks
  num_ctx: 32768                      # Larger context window for richer evidence
  top_p: 0.9                          # Nucleus sampling for more natural text
  repeat_penalty: 1.15                # Reduce repetitive phrasing

# ------ Ollama Recommended Env Vars ------
# Before running `ollama serve`, set:
#   export OLLAMA_FLASH_ATTENTION=1
#   export OLLAMA_NUM_PARALLEL=3
#   export OLLAMA_CONTEXT_LENGTH=32768

# ------ PubMed / Entrez Retrieval ------
retrieval:
  email: "your_email@example.com"
  api_key: ""
  max_results: 2000                   # Can handle more studies
  batch_size: 100
  db: "pubmed"

# ------ Deduplication (larger, more accurate model) ------
deduplication:
  model: "all-mpnet-base-v2"          # 768-dim vs 384-dim (MiniLM)
  similarity_threshold: 0.95

# ------ Content Analyzer (stages 3-6) ------
content_analyzer:
  model: "all-mpnet-base-v2"
  chunk_max_tokens: 600               # Larger chunks (4060: 400)
  chunk_overlap: 100                  # More overlap (4060: 50)
  similarity_threshold: 0.25
  top_k_tags: 5                       # More tags per chunk (4060: 3)
  embedding_batch_size: 256           # 8x larger batches (4060: 32)

# ------ Review Writer (stages 7-8) ------
review_writer:
  top_k_evidence: 25                  # More evidence per section (4060: 10)
  parallel_workers: 3                 # 3 sections at once (32B uses less VRAM than 70B)
  max_retries: 2                      # Retry failed sections
  language: "pt"                      # Output language: "pt", "en", or "es"
  evidence_pre_summarize: true        # Summarize evidence before section writing
  two_pass_writing: true              # Draft → Polish pipeline

# ------ Post-Processing Synthesis Refinement ------
post_processing:
  enabled: true
  parallel_workers: 3                 # Match review_writer parallelism
  max_retries: 2
  preserve_v1: true                   # Keep v1 backup file

# ------ Screening ------
screening:
  threshold_include: 0.75
  threshold_exclude: 0.25

# ------ Synthesis ------
synthesis:
  min_studies_for_meta: 3
  confidence_level: 0.95

# ------ Paths (relative to project root) ------
paths:
  database: "data/raw/studies.db"
  prisma: "data/results/prisma.json"
  extracted: "data/processed/extracted_data.json"
  risk_of_bias: "data/processed/risk_of_bias.json"
  pico: "data/processed/pico.json"
  manuscript: "data/results/manuscript.tex"
  audit_log: "data/results/audit.log"
  pipeline_state: "data/results/pipeline_state.json"
  forest_plot: "data/results/forest_plot.png"

# ------ Multi-Agent System ------
multi_agent:
  enabled: false                     # set to true or use --multi-agent flag
  max_iterations: 3                  # max write→review iterations per section
  quality_threshold: 7.0             # minimum review score to accept a section
  parallel_themes: 2                 # concurrent theme processing
  write_retries: 1                   # retries per LLM write call
  debate_enabled: true               # enable debate for controversial themes
  debate_controversy_threshold: 2    # min contradictions to trigger debate
  agent_models:                      # per-agent model overrides (null = use global)
    extraction: ~                    # lightweight — uses default qwen3:8b
    mapping: ~
    critical: "mixtral:8x22b"        # complex reasoning for comparative analysis
    synthesis: "mixtral:8x22b"       # complex reasoning for thesis + agenda
    writing: ~
    review: ~
    debate: "mixtral:8x22b"          # complex reasoning for debate sections

# ------ Versioning / Reproducibility ------
version:
  pipeline: "1.3.0"
  config_hash: ""
